{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN_SOKOBAN_challenge.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5L88ZFOfGsYB"
      },
      "source": [
        "Based on https://ymd_h.gitlab.io/ymd_blog/posts/gym_on_google_colab_with_gnwrapper/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1u9QVVsShC9X"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEHR2Ui-lo8O"
      },
      "source": [
        "!sudo apt-get install -y xvfb ffmpeg x11-utils\n",
        "!pip install -q 'gym==0.10.11'\n",
        "!pip install -q 'imageio==2.4.0'\n",
        "!pip install -q PILLOW\n",
        "!pip install -q 'pyglet==1.3.2'\n",
        "!pip install -q pyvirtualdisplay\n",
        "!pip install -q tf-agents\n",
        "!pip install colabgymrender\n",
        "!pip install gym_sokoban\n",
        "!pip install gym\n",
        "!apt-get install python-opengl -y\n",
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install python-opengl -y\n",
        "!apt install xvfb -y\n",
        "!pip install piglet\n",
        "import gym\n",
        "import gym_sokoban\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
        "from colabgymrender.recorder import Recorder\n",
        "from IPython import display as ipythondisplay\n",
        "from pyvirtualdisplay import Display\n",
        "Display().start()\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display as ipythondisplay"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbOqA9qYNYBo"
      },
      "source": [
        "env = gym.make(\"PushAndPull-Sokoban-v2\")\n",
        "screen = env.render(mode='rgb_array')\n",
        "plt.imshow(screen)\n",
        "observation = env.reset()\n",
        "ipythondisplay.clear_output(wait=True)\n",
        "ipythondisplay.display(plt.gcf())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CijGYcvy-vS8"
      },
      "source": [
        "observation.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkuGDLk_3KDK"
      },
      "source": [
        "from skimage import io, color\n",
        "\n",
        "lina_gray = color.rgb2gray(observation[16:96, 16:96])\n",
        "print(lina_gray.shape)\n",
        "plt.imshow(lina_gray)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cffjAnsrGGO9"
      },
      "source": [
        "d = Display()\n",
        "d.start()\n",
        "\n",
        "screeens = list()\n",
        "directory = './video'\n",
        "env = gym.wrappers.Monitor(env, \"recording\",force=True)\n",
        "env.render(mode='human')\n",
        "env.reset()\n",
        "for _ in range(2):\n",
        "    action = env.action_space.sample()\n",
        "    #print(action)\n",
        "    print(action)\n",
        "    state, reward, done, info = env.step(action) \n",
        "    print(state.shape, reward, done, info)\n",
        "    screen = env.render(mode='rgb_array')\n",
        "    screeens.append(screen)\n",
        "    #plt.imshow(screen)\n",
        "    #ipythondisplay.clear_output(wait=True)\n",
        "    #ipythondisplay.display(plt.gcf())\n",
        "    if done:\n",
        "        env.reset()\n",
        "        print('done')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tRJVIphdjMB"
      },
      "source": [
        "plt.imshow(screen)\n",
        "ipythondisplay.clear_output(wait=True)\n",
        "ipythondisplay.display(plt.gcf())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIxIsoNq9R4R"
      },
      "source": [
        "import gym\n",
        "import random\n",
        "import os\n",
        "import numpy as np\n",
        "from collections      import deque\n",
        "from keras.models     import Sequential\n",
        "from keras.layers     import Dense, Conv2D, Flatten\n",
        "from keras.optimizers import Adam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DS2nfImW9sBB"
      },
      "source": [
        "class Agent():\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.weight_backup      = \"SOKOBAN_weight.h5\"\n",
        "        self.state_size         = state_size\n",
        "        self.action_size        = action_size\n",
        "        self.memory             = deque(maxlen=2000)\n",
        "        self.learning_rate      = 0.001 # @param {type:\"number\"}\n",
        "        self.gamma              = 0.95 # @param {type:\"number\"}\n",
        "        self.exploration_rate   = 1.0# @param {type:\"number\"}\n",
        "        self.exploration_min    = 0.01# @param {type:\"number\"}\n",
        "        self.exploration_decay  = 0.995 # @param {type:\"number\"}\n",
        "        self.brain              = self._build_model()\n",
        "\n",
        "    def _build_model(self):\n",
        "        # Neural Net for Deep-Q learning Model\n",
        "        model = Sequential()\n",
        "        model.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=self.state_size, padding='same'))\n",
        "        model.add(Dense(32, activation='relu'))\n",
        "        #model.add(Dense(16, activation='relu'))\n",
        "        #model.add(Flatten())\n",
        "        model.add(Dense(self.action_size, activation='linear'))\n",
        "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
        "        '''if os.path.isfile(self.weight_backup):\n",
        "            model.load_weights(self.weight_backup)\n",
        "            self.exploration_rate = self.exploration_min'''\n",
        "        return model\n",
        "\n",
        "    def save_model(self):\n",
        "            self.brain.save(self.weight_backup)\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.rand() <= self.exploration_rate:\n",
        "            return random.randrange(self.action_size)\n",
        "        act_values = self.brain.predict(state)\n",
        "        return np.argmax(act_values[0][0][0])\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done, info):\n",
        "        self.memory.append((state, action, reward, next_state, done, info))\n",
        "\n",
        "    def replay(self, sample_batch_size):\n",
        "        if len(self.memory) < sample_batch_size:\n",
        "            return\n",
        "        sample_batch = random.sample(self.memory, sample_batch_size)\n",
        "        for state, action, reward, next_state, done, info in sample_batch:\n",
        "            target = reward\n",
        "\n",
        "            if not done:\n",
        "              target = reward + self.gamma * np.amax(self.brain.predict(next_state)[0])\n",
        "            target_f = self.brain.predict(state)\n",
        "            target_f[0][action] = target\n",
        "            self.brain.fit(next_state, target_f, epochs=1, verbose=0)\n",
        "        if self.exploration_rate > self.exploration_min:\n",
        "            self.exploration_rate *= self.exploration_decay"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-oKIm169wfH"
      },
      "source": [
        "class Sokoban:\n",
        "    def __init__(self):\n",
        "        self.sample_batch_size = 64 # @param {type:\"integer\"}\n",
        "        self.episodes          =  300000# @param {type:\"integer\"}\n",
        "        self.env               = gym.make('PushAndPull-Sokoban-v2')\n",
        "\n",
        "        self.state_size        =  (80, 80, 1)\n",
        "        self.action_size       = self.env.action_space.n\n",
        "        self.agent             = Agent(self.state_size, self.action_size)\n",
        "\n",
        "\n",
        "    def run(self):\n",
        "        try:\n",
        "            for index_episode in range(self.episodes):\n",
        "                state = self.env.reset()\n",
        "                state = color.rgb2gray(state[16:96, 16:96]).reshape(1, 80, 80, 1)\n",
        "\n",
        "                done = False\n",
        "                index = 0\n",
        "                total_reward = list()\n",
        "                while not done:\n",
        "#                    self.env.render()\n",
        "                    action = self.agent.act(state)\n",
        "                    next_state, reward, done, info = self.env.step(action)\n",
        "                    reward = reward*10\n",
        "\n",
        "                    total_reward.append(reward)\n",
        "                    next_state = color.rgb2gray(next_state[16:96, 16:96]).reshape(1, 80, 80, 1)\n",
        "                    self.agent.remember(state, action, reward, next_state, done, info)\n",
        "                    state = next_state\n",
        "                    index += 1\n",
        "                print(\"Episode {}# steps: {}, total reward: {}, last reward: {}, rewards_list: {}\".format(index_episode, index + 1, sum(total_reward), reward, total_reward))\n",
        "                self.agent.replay(self.sample_batch_size)\n",
        "        finally:\n",
        "            self.agent.save_model()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHdquP9z96kR"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    sokoban = Sokoban()\n",
        "    sokoban.run()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}